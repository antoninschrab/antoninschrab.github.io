@unpublished{schrab2021mmd,
  title = {{MMD} Aggregated Two-Sample Test},
  author = {Antonin Schrab and Ilmun Kim and Mélisande Albert and Béatrice Laurent and Benjamin Guedj and Arthur Gretton},
  year = {2021},
  note = "Submitted.",
  abstract = {We propose a novel nonparametric two-sample test based on the Maximum Mean Discrepancy (MMD), which is constructed by aggregating tests with different kernel bandwidths. This aggregation procedure, called MMDAgg, ensures that test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We work in the non-asymptotic framework, and prove that our aggregated test is minimax adaptive over Sobolev balls. Our guarantees are not restricted to a specific kernel, but hold for any product of one-dimensional translation invariant characteristic kernels which are absolutely and square integrable. Moreover, our results apply for popular numerical procedures to determine the test threshold, namely permutations and the wild bootstrap. Through numerical experiments on both synthetic and real-world datasets, we demonstrate that MMDAgg outperforms alternative state-of-the-art approaches to MMD kernel adaptation for two-sample testing.},
  keywords = {Hypothesis test},
  url = {https://arxiv.org/abs/2110.15073},
  url_PDF = {https://arxiv.org/pdf/2110.15073.pdf},
  url_Code = {https://github.com/antoninschrab/mmdagg-paper},
  url_Slides = {https://antoninschrab.github.io/files/Slides_MMDAgg_KSDAgg_long.pdf},
  url_Poster = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
  eprint = {2110.15073},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}

@unpublished{schrab2022ksd,
  title = {{KSD} Aggregated Goodness-of-fit Test},
  author = {Antonin Schrab and Benjamin Guedj and Arthur Gretton},
  year = {2022},
  note = "Submitted.",
  abstract = {We investigate properties of goodness-of-fit tests based on the Kernel Stein Discrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg, which aggregates multiple tests with different kernels. KSDAgg avoids splitting the data to perform kernel selection (which leads to a loss in test power), and rather maximises the test power over a collection of kernels. We provide theoretical guarantees on the power of KSDAgg: we show it achieves the smallest uniform separation rate of the collection, up to a logarithmic term. KSDAgg can be computed exactly in practice as it relies either on a parametric bootstrap or on a wild bootstrap to estimate the quantiles and the level corrections. In particular, for the crucial choice of bandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such as median or standard deviation) or to data splitting. We find on both synthetic and real-world data that KSDAgg outperforms other state-of-the-art adaptive KSD-based goodness-of-fit testing procedures.},
  keywords = {Hypothesis test},
  url = {https://arxiv.org/abs/2202.00824},
  url_PDF = {https://arxiv.org/pdf/2202.00824.pdf},
  url_Code = {https://github.com/antoninschrab/ksdagg-paper},
  url_Slides = {https://antoninschrab.github.io/files/Slides_MMDAgg_KSDAgg_long.pdf},
  url_Poster = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
  eprint = {2202.00824},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}
