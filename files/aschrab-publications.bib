@unpublished{schrab2021mmd,
  title = {{MMD} Aggregated Two-Sample Test},
  author = {Antonin Schrab and Ilmun Kim and Mélisande Albert and Béatrice Laurent and Benjamin Guedj and Arthur Gretton},
  year = {2021},
  note = "Submitted.",
  abstract = {We propose a novel nonparametric two-sample test based on the Maximum Mean Discrepancy (MMD), which is constructed by aggregating tests with different kernel bandwidths. This aggregation procedure, called MMDAgg, ensures that test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We work in the non-asymptotic framework, and prove that our aggregated test is minimax adaptive over Sobolev balls. Our guarantees are not restricted to a specific kernel, but hold for any product of one-dimensional translation invariant characteristic kernels which are absolutely and square integrable. Moreover, our results apply for popular numerical procedures to determine the test threshold, namely permutations and the wild bootstrap. Through numerical experiments on both synthetic and real-world datasets, we demonstrate that MMDAgg outperforms alternative state-of-the-art approaches to MMD kernel adaptation for two-sample testing.},
  keywords = {Hypothesis testing},
  url = {https://arxiv.org/abs/2110.15073},
  url_PDF = {https://arxiv.org/pdf/2110.15073.pdf},
  url_Code = {https://github.com/antoninschrab/mmdagg-paper},
  url_Slides_1 = {https://antoninschrab.github.io/files/Slides_MMDAgg_KSDAgg_long.pdf},
  url_Slides_2 = {https://antoninschrab.github.io/files/Slides_handout-31-05-22.pdf},
  url_Poster_1 = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
  url_Poster_2 = {https://antoninschrab.github.io/files/Poster_MMDAgg.pdf},
  url_Video_1 = {https://youtu.be/F0VOCrAf5_M},
  url_Video_2 = {https://youtu.be/OWh6Hj10wsY},
  eprint = {2110.15073},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}

@inproceedings{schrab2022ksd,
  title = {{KSD} Aggregated Goodness-of-fit Test},
  author = {Antonin Schrab and Benjamin Guedj and Arthur Gretton},
  booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022},
  editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year = {2022},
  abstract = {We investigate properties of goodness-of-fit tests based on the Kernel Stein Discrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg, which aggregates multiple tests with different kernels. KSDAgg avoids splitting the data to perform kernel selection (which leads to a loss in test power), and rather maximises the test power over a collection of kernels. We provide theoretical guarantees on the power of KSDAgg: we show it achieves the smallest uniform separation rate of the collection, up to a logarithmic term. KSDAgg can be computed exactly in practice as it relies either on a parametric bootstrap or on a wild bootstrap to estimate the quantiles and the level corrections. In particular, for the crucial choice of bandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such as median or standard deviation) or to data splitting. We find on both synthetic and real-world data that KSDAgg outperforms other state-of-the-art adaptive KSD-based goodness-of-fit testing procedures.},
  keywords = {Hypothesis testing},
  url = {https://arxiv.org/abs/2202.00824},
  url_PDF = {https://arxiv.org/pdf/2202.00824.pdf},
  url_Code = {https://github.com/antoninschrab/ksdagg-paper},
  url_Slides_1 = {https://antoninschrab.github.io/files/Slides_MMDAgg_KSDAgg_long.pdf},
  url_Slides_2 = {https://antoninschrab.github.io/files/Slides_handout-31-05-22.pdf},
  url_Poster_1 = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
  url_Poster_2 = {https://antoninschrab.github.io/files/Poster-03-09-22.pdf},
  url_Long_Video = {https://youtu.be/F0VOCrAf5_M},
  url_Short_Video = {https://youtu.be/OWh6Hj10wsY},
  eprint = {2202.00824},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}

@inproceedings{schrab2022efficient,
  title = {Efficient Aggregated Kernel Tests using Incomplete {$U$}-statistics},
  author = {Antonin Schrab and Ilmun Kim and Benjamin Guedj and Arthur Gretton},
  booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022},
  editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year = {2022},
  abstract = {We propose a series of computationally efficient, nonparametric tests for the two-sample, independence and goodness-of-fit problems, using the Maximum Mean Discrepancy (MMD), Hilbert Schmidt Independence Criterion (HSIC), and Kernel Stein Discrepancy (KSD), respectively. Our test statistics are incomplete $U$-statistics, with a computational cost that interpolates between linear time in the number of samples, and quadratic time, as associated with classical $U$-statistic tests. The three proposed tests aggregate over several kernel bandwidths to detect departures from the null on various scales: we call the resulting tests MMDAggInc, HSICAggInc and KSDAggInc. For the test thresholds, we derive a quantile bound for wild bootstrapped incomplete $U$- statistics, which is of independent interest. We derive uniform separation rates for MMDAggInc and HSICAggInc, and quantify exactly the trade-off between computational efficiency and the attainable rates: this result is novel for tests based on incomplete $U$-statistics, to our knowledge. We further show that in the quadratic-time case, the wild bootstrap incurs no penalty to test power over more widespread permutation-based approaches, since both attain the same minimax optimal rates (which in turn match the rates that use oracle quantiles). We support our claims with numerical experiments on the trade-off between computational efficiency and test power. In the three testing frameworks, we observe that our proposed linear-time aggregated tests obtain higher power than current state-of-the-art linear-time kernel tests.},
  keywords = {Hypothesis testing},
  url = {https://arxiv.org/abs/2206.09194},
  url_PDF = {https://arxiv.org/pdf/2206.09194.pdf},
  url_Code = {https://github.com/antoninschrab/agginc-paper},
  url_Slides = {https://antoninschrab.github.io/files/Slides_handout-31-05-22.pdf},
  url_Poster = {https://antoninschrab.github.io/files/Poster-03-09-22.pdf},
  eprint = {2206.09194},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}

@article{schrab2022discussion,
  author = {Antonin Schrab and Wittawat Jitkrittum and Zolt\'an Szab\'o and Dino Sejdinovic and Arthur Gretton},
  title = {Discussion of `{M}ultiscale {F}isher's Independence Test for Multivariate Dependence'},
  journal = {Biometrika},
  volume = {109},
  number = {3},
  pages = {597-603},
  year = {2022},
  month = {08},
  issn = {1464-3510},
  doi = {10.1093/biomet/asac028},
  url = {https://doi.org/10.1093/biomet/asac028},
  eprint = {https://academic.oup.com/biomet/article-pdf/109/3/597/45512180/asac028.pdf},
  url_arXiv = {https://arxiv.org/pdf/2206.11142},
  abstract = {We discuss how MultiFIT, the Multiscale Fisher's Independence Test for Multivariate Dependence proposed by Gorsky and Ma (2022), compares to existing linear-time kernel tests based on the Hilbert-Schmidt independence criterion (HSIC). We highlight the fact that the levels of the kernel tests at any finite sample size can be controlled exactly, as it is the case with the level of MultiFIT. In our experiments, we observe some of the performance limitations of MultiFIT in terms of test power.},
  keywords = {Hypothesis testing},
  eprint = {2206.11142},
  archivePrefix = {arXiv},
  primaryClass = {stat.ME}
}

@unpublished{biggs2023mmdfuse,
  title = {{MMD-FUSE}: {L}earning and Combining Kernels for Two-Sample Testing Without Data Splitting},
  author = {Antonin Schrab and Ilmun Kim and Benjamin Guedj and Arthur Gretton},
  year = {2023},
  note = "Submitted.",
  abstract = {We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.},
  keywords = {Hypothesis testing},
  url = {https://arxiv.org/abs/2306.08777},
  url_PDF = {https://arxiv.org/pdf/2306.08777.pdf},
  url_Code = {https://github.com/antoninschrab/mmdfuse-paper},
  url_Slides = {https://antoninschrab.github.io/files/Slides_MMDFUSE.pdf},
  eprint = {2306.08777},
  archivePrefix = {arXiv},
  primaryClass = {stat.ML}
}
